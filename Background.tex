\chapter{Background} \label{chap:Background}

\setcounter{secnumdepth}{3}
\renewcommand{\thesubsubsection}{\Alph{subsubsection}}

%\titleformat{\subsubsection}
%{\selectfont}{\thesubsubsection}{1em}{}

%Give background about polar codes and their adoption in 5G.

Polar codes were introduced by Arikan in his seminal work \cite{Arikan}. Polar codes are the class of capacity achieving codes. In the past decade, polar codes have sparked a interest from both academia and industrial resulting in significant research work in improving performance. The 5\textsuperscript{th} generation wireless systems (5G)  standardization has adopted polar codes for uplink and downlink control information for the enhanced mobile broadband (eMBB). Polar codes are also considered as the potential coding schemes for two other frameworks of 5G, namely ultra-reliable-low-latency (URLLC) and massive machine-type communications (mMTC).


Polar codes achieve capacity asymptotically for of memoryless channel. Although polar codes are the first theoretically capacity achieving codes with an explicit construction, capacity is approached only asymptotically their performance is suboptimal compared to LDPC or Turbo codes at short block lengths with successive cancellation decoding (SCD). \cite{SCL} Presents the improved version of SCD called \emph{successive cancellation list decoder(SCLD)}.


The construction of polar codes involves the identification of channel reliability values, information bits are placed in the K high reliable bit indices out of N positions and remaining bits are set to zero then these N bits are passed through a polar encoding circuit to get the encoded bits. Selection is of reliability indices is done based on the code length and channel signal-to-noise ratio. Due to varying code length and channel conditions in 5G systems significant effort has been put to identify reliability indices which have good error correction performance over multiple code length and channel parameters.

\section{Background of Polar codes} 
This section introduces the basic mathematical foundations of the polar codes. In particular, about the frozen set design, encoding and decoding. Different decoding algorithms are introduced. Mainly Successive Cancellation Decoding(SCD), Successive Cancellation List Decoding(SCLD) and CRC aided Successive Cancellation List Decoding(CA-SCLD). Examples of encoding and decoding with different algorithms are presented for better understanding.

%\subsection{Polar codes definition} \label{polarCodesDefn}

Mathematical foundations of polar codes lay on the polarization effect of the matrix \cite{Arikan}. $ k = \big[\begin{smallmatrix} 1 & 0 \\ 1 & 1 \end{smallmatrix}$\big] also called Arikan matrix. Polar codes are $(N,K)$ linear block codes of size $N = 2^{n}$ where $n$ being a natural number. $N$ is the block length of the code and $K$ is the number of information bits.  $N$-bit vector $U$ contains $K$ information and $N-K$ frozen bits which are set to known value mostly zeros. These bits are then multiplied with the generator matrix constructed from Kronecker power of Arikan kernel matrix.

For example $n=3$, block length $N$ becomes $8$ hence the generator matrix is \newline

$ k^{\otimes 3} = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\ 
1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\ 
1 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\ 
1 & 1 & 1 & 1 & 0 & 0 & 0 & 0\\ 
1 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\ 
1 & 1 & 0 & 0 & 1 & 1 & 0 & 0\\ 
1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\ 
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1
\end{bmatrix}$ \newline

where $k^{\otimes n}$ denotes the $n^{th}$ Kronecker power of $k$. The encoding process involves the multiplication of $N$-bit vector $U$ consisting of $K$ information bits and $N-K$ frozen bits with $k^{\otimes n}$.

\subsection{Polar code construction} \label{CodeConstruction}

In polar coding, first step is to identify the channel reliability values for a particular block length, this step is also called as polar code construction. Basic idea of polar coding is to manufacture out of $N$ independent copies of given binary discrete memoryless channel create a fraction of channels which are either completely noiseless or noisy.This process of creating extremal channels is called channel polarization. As $N\to\infty$ fraction of noiseless channels approaches capacity of channel. Estimating reliability indices channels is carried by considering the Bhattacharyya parameter which indicates the reliability of individual channel.
 
For a generic binary-input discrete memoryless channel (B-DMC) which is represented as $W \colon \mathcal{X} \to \mathcal{Y}$ with input alphabet $\mathcal{X}$, output alphabet $\mathcal{Y}$ and transition probabilities given by $W(y|x),x \in \mathcal{X}, y \in \mathcal{Y}$.

Bhattacharyya parameter is given by 
\begin{equation}
	Z(W) \triangleq \sum_{y \in \mathcal{Y}} \sqrt{W(y|0)W(y|1)}
\end{equation}

Bhattacharyya parameter indicates how unreliable the channel is, It is easy see that $Z(W)$ takes values between $[0,1]$ better the channel smaller is $Z(W)$. Polarization creates channels with $Z(W)$ of 0 or 1.

\textbf{Here Give one example channel polarization for BEC channel and show the evolution of channel reliabilities. May include a insert a picture with evolution of channel reliabilities plot.}


\subsection{Encoding} \label{polarEncoding}
As explained in the section \ref{CodeConstruction} code construction is done. Information bits are placed in the most reliable bit indices position and non reliable bit positions are called frozen bits whose values are set to zeros. This $N$-bit vector $U$ is multiplied with generator matrix obtained by the Kronecker power of Arikan kernel matrix. Multiplying with generator matrix can also be represented as circuit form. for $n = 3$ block length $N$ becomes 8 for such a case encoding circuit looks as shown in the  Figure ~\ref{fig:encoderCircuit}. \newline

\begin{figure}[h]
	\centering
	\includegraphics{./figures/ButterFlyCircuit.pdf}
	\caption{Butterfly circuit representing Arikan Kernel matrix}
	\label{fig:butterFlyCicuit}
\end{figure}


\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{./figures/EncodingCircuitStages.pdf}
	\caption{Polar encoder in circuit form for $N = 8$}
	\label{fig:encoderCircuit}
\end{figure}

the grayed locations are the frozen bit indices which are set to zero, in remaining positions information bits are inserted. Output of the circuit is a code word which is transmitted over the channel.

Lets consider an example with $N = 8$ and $K = 4$, rate of this code is $R = K/N = 1/2$. As given in the figure frozen bit indices are ${\{0,1,2,4\}}$ remaining indices contain information bits.  Let the infomation which needs to transmitted be \{1,1,0,0\}, then after placing information bits at reliable channel positions the vector $U$ becomes \{0,0,0,1,0,1,0,0\}. After passing through the polar encoding circuit shown in Figure ~\ref{fig:encoderCircuit}. The result of encoder are \{0,0,1,1,1,1,0,0\}. These encoded bits are transmitted over the channel.

The encoding circuit is nothing but recursive application of the transformation represented by the butterfly circuit shown in the figure ~\ref{fig:butterFlyCicuit}. One butterfly unit can transform two uncorrelated bits $(a,b)$ into two correlated output bits $(a\oplus b,b)$ which is dominated by $k$ the Arikan Matrix. This corresponds to two channel polarization. In the above example reliability of the $u_{1}$ is increased compared to the $u_{0}$ channel. This operation recursively applied to whole code word results in the circuit shown in the figure ~\ref{fig:encoderCircuit}. Code word splits into two parts in stage-1, each of which again splits into two parts in stage-2 and so on, until one reaches to single source bit $u_{i}$ in stage-1. So the process of polar encoding for $N = 8$ involves three stages of butterfly operations. Generally for a given code length $N=2^{n}$, the polar encoding consists of stages each with $N/2$ butterfly operations, which results in an encoding complexity of $O(N\log(N))$.


\TODO{If you find any interesting, useful information for better understanding of the polar encoding, don't hesitate to include here.}

\subsection{Decoding}
As we saw in the section ~\ref{polarEncoding}, repetitive application of butterfly operation during the encoding introduces correlation between the source bits. At the receiver, correlation between the source bits is exploited to estimate the transmitted codeword. Utilizing the high correlation between the source bits forms the central idea basic polar decoding algorithm called \emph{Successive Cancellation (SC)}. This method sequentially decodes each of the bits and takes previously estimated value to account for estimating the next bit. This sequential decoding exploits correlation between the source bits which as introduced during polar encoding process. Due sequential nature of SC decoding, decoder has more latency which is unacceptable in mission critical applications. Improving the basic decoding algorithm has attracted lot of researchers in academia and industry. These improvements are mainly directed towards two goals, one is reducing decoding latency in other words high throughput and second is improving the error correction performance. Significant reduction in decoding latency is achieved by \cite{SSC} and \cite{fastSSC}. In this work, instead decoding sequentially individual bit, special nodes are identified which can be decoded in parallel. Although polar codes are the first theoretically capacity achieving codes with explicit construction, their error correction performance at short block lengths is not comparable with that of LDPC or Turbo codes. This behavior can be better explained by the way decoding is performed. As presented earlier SC algorithm works by sequentially decoding individual bits and using information of previously decoded bits for estimating next bit. The issue with algorithm is if the previously decoded bit is wrong, then there is no way of correcting this bit.

To overcome this problem \cite{SCL} presents a improved version of the SC algorithm called Successive Cancellation List Decoding (SCL). The basic idea is instead of deciding a value of bit $u_{i}$ it takes both options, this results in two decoding paths for every bit, so to avoid exponential growth of complexity decoding candidates are restricted to $L$ the list size. At the end of decoding, most probable candidate is chosen from list. Performance of polar codes with SCL is still not as good as LDPC or Turbo codes at small and moderate block lengths. Polar code concatenated with CRC as outer code beats the LDPC codes of similar block length \cite{SCL}. In this section, each of the algorithms are presented briefly with an example for quick understanding.

\paragraph{\emph{A. Successive Cancellation Decoding (SC)}}  \label{SC}
The recursive SC decoder is basic algorithm presented in \cite{Arikan} for decoding polar codes.  SC decoder is inherently sequential, it estimates $\hat{u_{i}}$ of $u_{i}$ by using channel observation $y^{N}_{1}$ and all the previously decoded bits $\hat{u}_{1}^{i-1}$. If $u_{i}$ is frozen bit, the decoder assigns $\hat{u_{i}}$ to known value (mostly zero). If $u_{i}$ is an information bit, the decoder waits for all the previous bits to compute the decoding metric. It may be one of three different type of metrics: \newline

$\bullet$ log-likelihood ratio (LLR) where


\begin{equation}
L_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}) = \ln{\Bigg(\frac{W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}|u_{i} = 0)} {W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}^{i-1}}|u_{i} = 1)}\Bigg)};
\end{equation}

$\bullet$ likelihood ratio (LR) where 

\begin{equation}
LR_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}) = \Bigg(\frac{W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}|u_{i} = 0)} {W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}^{i-1}}|u_{i} = 1)}\Bigg);
\end{equation}

$\bullet$ log-likelihood (LL) where 

\begin{equation}
LL(y_{1}^{N},\hat{u_{1}}^{i-1}) = \Big[ln\Big(W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}|u_{i} = 0)\Big), ln\Big(W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}^{i-1}}|u_{i} = 1)\Big)\Big];
\end{equation}


Decoding metric computed from LLR's exhibit better numerical stability than those from LR's or LL's, so we have used the LLR's metric throughout this work. There are different ways to view and understand the operation of SC decoder. In work decoding is viewed as message passing algorithm on an binary tree with $\log(N)$ levels. Decoding is performed by traversing a tree from root to leaf node. Process of decoding involves check node(CN), variable node (VN) operations and threshold detection at leaf node. Decoder receives a LLR value for every bit which needs to decoded (including both frozen and information bits), in other words for code with block length $N$ SC decoder receives $N$ LLR values. Decoding process estimates the bits $\hat{u}_{i} $  where $i = 0,1,2...,(N-1)$. The decoding tree for $N = 8$ looks as shown in the Figure ~\ref{fig:decodingTree}.

\begin{figure}[h]
	\centering
	\includegraphics{./figures/decodingTree.pdf}
	\caption{Decoding tree}
	\label{fig:decodingTree}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics{./figures/messagePassingDiaS.pdf}
	\caption{Local Decoder}
	\label{fig:msgPassingDia}
\end{figure}

In a decoding  tree, the messages to left child node computed with CN and to the right with VN operation.  Figure ~\ref{fig:msgPassingDia} shows how the messages are exchanged in a local component decoder.

The CN and VN operations in LLR domain are given by following equations:

$\bullet$ Check Node (CN) operation

\begin{equation} \label{cnop}
	\alpha_{v_{l}}[i] = \alpha_{v}[i] + \alpha_{v}[i + N_{v}/2]
\end{equation}

$\bullet$ Variable Node (VN) operation

\begin{equation} \label{vnop}
\alpha_{v_{r}}[i] = \alpha_{v}[i + N_{v}/2] + (1 - 2\beta_{v_{l}}[i]) * \alpha_{v}[i]
\end{equation}

After decoding is done at both right and left child nodes the bits are combined at common parent node. the bit combining operation is given by the following equation.

\begin{equation*} \label{bitCombination}
\beta_{v}[i] = \begin{cases}
				\beta_{v_{l}}[i] \oplus \beta_{v_{r}}[i] & \text{if }i < N_{v}/2 \\
				\beta_{v_{r}}[i]
				\end{cases}
\end{equation*}

In Figure ~\ref{fig:msgPassingDia} and in equations ~\ref{cnop}, ~\ref{vnop}  $\alpha_{v}$, $\beta_{v}$ represent intermediate LLR values and estimated bits at local decoder respectively.


\TODO{provide an example of decoding with real LLR values for $N = 8$}

\paragraph{\emph{B. Improved Successive Cancellation Decoding (fast-SSC)}\newline}  \label{fastSSC} 
In basic SC algorithm decoding is performed sequentially, previously decoded values are used for decoding the present bit. Due to sequential nature of the decoder, decoding latency in high, which is undesirable. \cite{SSC} and \cite{fastSSC} try to identify special kind of nodes in a decoder tree which can immediately decoded without traversing till the end of tree. \cite{SSC} tries to identify node with all information bits or all frozen bits. These nodes are called rate-one($R1$) and rate-zero ($R0$) nodes respectively. The $R1$ node can be decoded by taking hard decision and polar transform, since there is no extra information which can be gained from traversing the tree. Decoding of $R0$ node is not necessary since none of them are information bits, so all the bits are set known value which is known at transmitter and receiver which are mostly set to zero. \par Authors in \cite{fastSSC} extend the idea presented in \cite{SSC} by identifying two more kind of special nodes which can be decoded without traversing the tree single parity check ($SPC$) and repetition ($REP$) nodes. Both in \cite{SSC} and \cite{fastSSC} node type is identified based on the frozen pattern at the component decoder. For $SPC$ node, only one frozen bit is present at left most position. For $REP$ node, frozen pattern contains one information bit at right most position, remaining are frozen bits.

One such example, when frozen indices for $N = 8$ are $\{0,1,3,4\}$. The full decoding tree of Figure ~\ref{fig:decodingTree} gets reduced to tree with fewer nodes as shown in Figure ~\ref{fig:decodingTreePruned}. We can easily see that, in the original decoder tree number of nodes were $15$, in the pruned tree nodes is reduced to 7, which results in a significant reduction in number of computations and decoding latency.

\begin{figure}[h]
	\centering
	\includegraphics{./figures/decodingTreePruned.pdf}
	\caption{Pruned Decoder Tree}
	\label{fig:decodingTreePruned}
\end{figure}


\paragraph{\emph{C. List Decoding of Polar Codes (SCL)}\newline}  \label{SCL}


\section{Processor architecture background}
To better understand the bottlenecks and optimizations performed in software implementation of 5G FEC chain, it is necessary to equip ourselves with the fundamentals of processor architecture. This section gives necessary background about cache memory systems, instruction pipelining, branch predictors, Vector processing units and function calling mechanism etc.

\subsection{Cache memory} \label{cacheSection}
In the modern processors, fast memory called cache is used to reduce the average access time of main memory or RAM (Random Access Memory). Cache minimizes the number of accesses to RAM by storing frequently accessed location's data in it, hence avoiding huge penalty of reading data frequently from RAM which operates at a much lower frequency than the CPU. When memory location is accessed for the first time it is copied from RAM to cache, future accesses to same location are read/written only to cache. These fast memory is placed between RAM and processor. In modern processors instead of single cache, multi-level caches are present. The main idea behind having multi-level caches is that if the data is not found in first level then second level is checked if not then third level until the last level, still if the data is not found in the cache then RAM is accessed. This model significantly reduces the probability of accessing the RAM compared to having a single level cache. Complete memory hierarchy of the modern processors is shown in the figure  ~\ref{fig:memoryHierarchy} \cite{CMP}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{./figures/memoryHierarchy.pdf}
	\caption{Memory Hierarchy}
	\label{fig:memoryHierarchy}
\end{figure}

Above figure shows processor architecture with three level caches namely L1, L2 and L3. In the order of increasing access latency, reducing cost and increasing size. L1 cache is fastest, costliest and smallest among all caches. L2 is slower than L1 but faster than L3, has more capacity than L1 but smaller than L3 and cheaper than L1. Similar order applies to L3 cache. Data is mapped to either memory. If registers available are not enough in such a case data is stored in memory. If the data is not found in all caches then it results in cache miss which causes processor instruction execution to stop until data is fetched from RAM. Whenever the memory location is accessed for the first time it always results in cache miss. Present data modern processors provide special instructions to avoid these compulsory cache misses, these are called cache prefetch instructions which allow programmer to fetch data from cache before it is accessed, hence hiding the memory access latency. Some other techniques to avoid cache misses are Reusing the allocated memory as much as possible and bit packing/unpacking to reduce the required memory. All the above mentioned techniques namely using prefetch instructions ($\mathtt{PREFETCH}$) provided AMD EPYC processor, reusing the allocated memory and bit packing/unpacking are used reduce the memory access latency.

\subsection{Instruction pipelining and branch predictors}
Traditionally processors were designed to follow the steps fetch, decode, finally execute and then fetch the next instruction. Although these steps are sufficient to solve any problem in hand, it is very inefficient in terms of hardware utilization. When instruction is getting fetched decode and execution modules are idle, if instruction is in decode phase fetch and execute idle similarly in the execution phase. To overcome under utilization of hardware resources modern processors implement instruction pipelining concept, where if the current instruction is in decoding phase the next instruction will be concurrently fetched by the instruction fetch module. Pipelining mechanism increases the instruction throughput by significantly reducing CPI (Cycles per Instruction). Example of sequential and pipelined execution is shown in figure \ref{fig:pipeline} \cite{SoCT}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{./figures/pipeline_seq2_edited.pdf}
	\caption{Instruction pipelining}
	\label{fig:pipeline}
\end{figure}

Example shown in figure \ref{fig:pipeline} assumes only three phases of instruction execution. Modern processors divide instructions execution to nineteen plus phases, which allows running processor at much higher frequency due to reduced critical path delay. Maximum advantage of pipelining can only be exploited when there are no pipeline stalling or flushing which happen when there is a data dependency, cache misses or branch instructions. Biggest contributors to pipeline stalling are cache misses and branch instructions. As explained in \ref{cacheSection} can be reduced by using combination of different optimization techniques. Next culprit is branch instructions, whether to branch or not is decided only the at execution stage. By the time branching is decided many of the future instructions are fetched, if the decision is to jump then all the prefetched instructions must be flushed which introduces stall in the pipeline. To overcome this issue branch predictors are designed which pro actively fetch instructions from the address to branch will be taken, hence avoiding flushing of pipeline. Branch predictors function by storing the previous decisions on the branching whether it was taken or not, hence requires correct previous state to proactively fetch future instructions. This method reduces the pipeline caused due to looping type of code, by reducing pipeline flushes. For the scenarios where there are no looping instruction just if or if-else constructs branch predictors fail to correctly fetch the future instructions. These kind of scenarios can be minimized by avoiding branch instructions wherever possible and by providing hints to compiler built-in macros to reduce the branching by better placement of assembly instructions (also called instruction scheduling). One such macro is 
\begin{minted}{c++}
	long __builtin_expect(long EXP, long C); 
\end{minted}
which tells compiler to place more frequently executing part of the code just after branch instruction to minimize pipeline flushing. Following code snippet shows the typical usage.

\begin{minted}{c++}
#define likely(expr) __builtin_expect(!!(expr), 1)
if (likely(a > 1)) {
	//Frequently executing part, most of the cases a is greater than 1
	...
} else {
	//Rarely executing part, rarely a is less than 1
	...
}
\end{minted}

Another feature provided by modern processors is conditional move instruction ($\mathtt{CMOV}$). Compiler intelligently maps if statements to conditional move instructions. $\mathtt{CMOV}$ copies a particular value to register or memory based on the flags set. It is not vulnerable to branch-prediction failure since no branch instructions gets generated, hence avoiding pipeline flushing. Following listing in \ref{code:NaiveCMOVEg} and \ref{code:CMOVEEg} illustrates the feature.

\begin{code}
	\captionof{listing}{Naive method}
	\label{code:NaiveCMOVEg}
\begin{minted}{c++}
uint32_t bitMask = 0;
if(CRCLENGTH == 6) //If Crc6 needs be calculated, then change the mask.
  bitMask = 0x3F;
else
  bitMask = 0x7FF;
\end{minted}
\end{code}

\begin{code}
	\captionof{listing}{With $\mathtt{CMOV}$}
	\label{code:CMOVEEg}
\begin{minted}{c++}
uint32_t bitMask = 0x7FF; //Initializing with CRC11 mask.
if(CRCLENGTH == 6) //If Crc6 needs be calculated, then change the mask.
  bitMask = 0x3F;
\end{minted}
\end{code}

Both code achieve same results, however \ref{code:NaiveCMOVEg} has possibility of branch-prediction, for the \ref{code:CMOVEEg} compiler identifies conditional move construct and generates $\mathtt{CMOV}$ potentially avoiding pipeline flush. Optimizations such as minimizing branches, using built-in macros and using constructs which help compiler to identify pattern are utilized in this work.

\subsection{Vector processing units}
Vector processing units are special kind of multiple computational elements that perform same operation on multiple points simultaneously. Machines with vector processing units exploits data level parallelism but not concurrency. Same instruction operates on multiple data points in other words there are simultaneous computations but there is a single process. These special kind of instructions are also called SIMD (Single Instruction Multiple Data) in Flynn's taxonomy of parallel computers. These instructions are particularly useful when same operation needs to 


\subsection{Function calling mechanism}


\TODO{Here I need to explain the typical features which are important for understanding the latency contributors and how they can be resolved}

