\chapter{Background} \label{chap:Background}

\setcounter{secnumdepth}{3}
\renewcommand{\thesubsubsection}{\Alph{subsubsection}}

%\titleformat{\subsubsection}
%{\selectfont}{\thesubsubsection}{1em}{}

%Give background about polar codes and their adoption in 5G.

Polar codes were introduced by Arikan in his seminal work \cite{Arikan}. Polar codes are the class of capacity achieving codes. In the past decade, polar codes have sparked a interest from both academia and industrial resulting in significant research work in improving performance. The 5\textsuperscript{th} generation wireless systems (5G)  standardization has adopted polar codes for uplink and downlink control information for the enhanced mobile broadband (eMBB). Polar codes are also considered as the potential coding schemes for two other frameworks of 5G, namely ultra-reliable-low-latency (URLLC) and massive machine-type communications (mMTC).


Polar codes achieve capacity asymptotically for of memoryless channel. Although polar codes are the first theoretically capacity achieving codes with an explicit construction, capacity is approached only asymptotically their performance is suboptimal compared to LDPC or Turbo codes at short block lengths with successive cancellation decoding (SCD). \cite{SCL} Presents the improved version of SCD called \emph{successive cancellation list decoder(SCLD)}.


The construction of polar codes involves the identification of channel reliability values, information bits are placed in the K high reliable bit indices out of N positions and remaining bits are set to zero then these N bits are passed through a polar encoding circuit to get the encoded bits. Selection is of reliability indices is done based on the code length and channel signal-to-noise ratio. Due to varying code length and channel conditions in 5G systems significant effort has been put to identify reliability indices which have good error correction performance over multiple code length and channel parameters.

\section{Background of Polar codes} 
This section introduces the basic mathematical foundations of the polar codes. In particular, about the frozen set design, encoding and decoding. Different decoding algorithms are introduced. Mainly Successive Cancellation Decoding(SCD), Successive Cancellation List Decoding(SCLD) and CRC aided Successive Cancellation List Decoding(CA-SCLD). Examples of encoding and decoding with different algorithms are presented for better understanding.

%\subsection{Polar codes definition} \label{polarCodesDefn}

Mathematical foundations of polar codes lay on the polarization effect of the matrix \cite{Arikan}. $ k = \big[\begin{smallmatrix} 1 & 0 \\ 1 & 1 \end{smallmatrix}$\big] also called Arikan matrix. Polar codes are $(N,K)$ linear block codes of size $N = 2^{n}$ where $n$ being a natural number. $N$ is the block length of the code and $K$ is the number of information bits.  $N$-bit vector $U$ contains $K$ information and $N-K$ frozen bits which are set to known value mostly zeros. These bits are then multiplied with the generator matrix constructed from Kronecker power of Arikan kernel matrix.

For example $n=3$, block length $N$ becomes $8$ hence the generator matrix is \newline

$ k^{\otimes 3} = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\ 
1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\ 
1 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\ 
1 & 1 & 1 & 1 & 0 & 0 & 0 & 0\\ 
1 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\ 
1 & 1 & 0 & 0 & 1 & 1 & 0 & 0\\ 
1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\ 
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1
\end{bmatrix}$ \newline

where $k^{\otimes n}$ denotes the $n^{th}$ Kronecker power of $k$. The encoding process involves the multiplication of $N$-bit vector $U$ consisting of $K$ information bits and $N-K$ frozen bits with $k^{\otimes n}$.

\subsection{Polar code construction} \label{CodeConstruction}

In polar coding, first step is to identify the channel reliability values for a particular block length, this step is also called as polar code construction. Basic idea of polar coding is to manufacture out of $N$ independent copies of given binary discrete memoryless channel create a fraction of channels which are either completely noiseless or noisy.This process of creating extremal channels is called channel polarization. As $N\to\infty$ fraction of noiseless channels approaches capacity of channel. Estimating reliability indices channels is carried by considering the Bhattacharyya parameter which indicates the reliability of individual channel.
 
For a generic binary-input discrete memoryless channel (B-DMC) which is represented as $W \colon \mathcal{X} \to \mathcal{Y}$ with input alphabet $\mathcal{X}$, output alphabet $\mathcal{Y}$ and transition probabilities given by $W(y|x),x \in \mathcal{X}, y \in \mathcal{Y}$.

Bhattacharyya parameter is given by 
\begin{equation}
	Z(W) \triangleq \sum_{y \in \mathcal{Y}} \sqrt{W(y|0)W(y|1)}
\end{equation}

Bhattacharyya parameter indicates how unreliable the channel is, It is easy see that $Z(W)$ takes values between $[0,1]$ better the channel smaller is $Z(W)$. Polarization creates channels with $Z(W)$ of 0 or 1.

\textbf{Here Give one example channel polarization for BEC channel and show the evolution of channel reliabilities. May include a insert a picture with evolution of channel reliabilities plot.}


\subsection{Encoding} \label{polarEncoding}
As explained in the section \ref{CodeConstruction} code construction is done. Information bits are placed in the most reliable bit indices position and non reliable bit positions are called frozen bits whose values are set to zeros. This $N$-bit vector $U$ is multiplied with generator matrix obtained by the Kronecker power of Arikan kernel matrix. Multiplying with generator matrix can also be represented as circuit form. for $n = 3$ block length $N$ becomes 8 for such a case encoding circuit looks as shown in the  Figure ~\ref{fig:encoderCircuit}. \newline

\begin{figure}[h]
	\centering
	\includegraphics{./figures/ButterFlyCircuit.pdf}
	\caption{Butterfly circuit representing Arikan Kernel matrix}
	\label{fig:butterFlyCicuit}
\end{figure}


\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{./figures/EncodingCircuitStages.pdf}
	\caption{Polar encoder in circuit form for $N = 8$}
	\label{fig:encoderCircuit}
\end{figure}

the grayed locations are the frozen bit indices which are set to zero, in remaining positions information bits are inserted. Output of the circuit is a code word which is transmitted over the channel.

Lets consider an example with $N = 8$ and $K = 4$, rate of this code is $R = K/N = 1/2$. As given in the figure frozen bit indices are ${\{0,1,2,4\}}$ remaining indices contain information bits.  Let the infomation which needs to transmitted be \{1,1,0,0\}, then after placing information bits at reliable channel positions the vector $U$ becomes \{0,0,0,1,0,1,0,0\}. After passing through the polar encoding circuit shown in Figure ~\ref{fig:encoderCircuit}. The result of encoder are \{0,0,1,1,1,1,0,0\}. These encoded bits are transmitted over the channel.

The encoding circuit is nothing but recursive application of the transformation represented by the butterfly circuit shown in the figure ~\ref{fig:butterFlyCicuit}. One butterfly unit can transform two uncorrelated bits $(a,b)$ into two correlated output bits $(a\oplus b,b)$ which is dominated by $k$ the Arikan Matrix. This corresponds to two channel polarization. In the above example reliability of the $u_{1}$ is increased compared to the $u_{0}$ channel. This operation recursively applied to whole code word results in the circuit shown in the figure ~\ref{fig:encoderCircuit}. Code word splits into two parts in stage-1, each of which again splits into two parts in stage-2 and so on, until one reaches to single source bit $u_{i}$ in stage-1. So the process of polar encoding for $N = 8$ involves three stages of butterfly operations. Generally for a given code length $N=2^{n}$, the polar encoding consists of stages each with $N/2$ butterfly operations, which results in an encoding complexity of $O(N\log(N))$.


\textbf{If you find any interesting, useful information for better understanding of the polar encoding, don't hesitate to include here.} \newpage

\subsection{Decoding}
As we saw in the section ~\ref{polarEncoding}, repetitive application of butterfly operation during the encoding introduces correlation between the source bits. At the receiver, correlation between the source bits is exploited to estimate the transmitted codeword. Utilizing the high correlation between the source bits forms the central idea basic polar decoding algorithm called \emph{Successive Cancellation (SC)}. This method sequentially decodes each of the bits and takes previously estimated value to account for estimating the next bit. This sequential decoding exploits correlation between the source bits which as introduced during polar encoding process. Due sequential nature of SC decoding, decoder has more latency which is unacceptable in mission critical applications. Improving the basic decoding algorithm has attracted lot of researchers in academia and industry. These improvements are mainly directed towards two goals, one is reducing decoding latency in other words high throughput and second is improving the error correction performance. Significant reduction in decoding latency is achieved by \cite{SSC} and \cite{fastSSC}. In this work, instead decoding sequentially individual bit, special nodes are identified which can be decoded in parallel. Although polar codes are the first theoretically capacity achieving codes with explicit construction, their error correction performance at short block lengths is not comparable with that of LDPC or Turbo codes. This behavior can be better explained by the way decoding is performed. As presented earlier SC algorithm works by sequentially decoding individual bits and using information of previously decoded bits for estimating next bit. The issue with algorithm is if the previously decoded bit is wrong, then there is no way of correcting this bit.

To overcome this problem \cite{SCL} presents a improved version of the SC algorithm called Successive Cancellation List Decoding (SCL). The basic idea is instead of deciding a value of bit $u_{i}$ it takes both options, this results in two decoding paths for every bit, so to avoid exponential growth of complexity decoding candidates are restricted to $L$ the list size. At the end of decoding, most probable candidate is chosen from list. Performance of polar codes with SCL is still not as good as LDPC or Turbo codes at small and moderate block lengths. Polar code concatenated with CRC as outer code beats the LDPC codes of similar block length \cite{SCL}. In this section, each of the algorithms are presented briefly with an example for quick understanding.

\paragraph{\emph{A. Successive Cancellation Decoding (SC)}}  \label{SC}
The recursive SC decoder is basic algorithm presented in \cite{Arikan} for decoding polar codes.  SC decoder is inherently sequential, it estimates $\hat{u_{i}}$ of $u_{i}$ by using channel observation $y^{N}_{1}$ and all the previously decoded bits $\hat{u}_{1}^{i-1}$. If $u_{i}$ is frozen bit, the decoder assigns $\hat{u_{i}}$ to known value (mostly zero). If $u_{i}$ is an information bit, the decoder waits for all the previous bits to compute the decoding metric. It may be one of three different type of metrics: \newline

$\bullet$ log-likelihood ratio (LLR) where


\begin{equation}
L_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}) = \ln{\Bigg(\frac{W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}|u_{i} = 0)} {W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}^{i-1}}|u_{i} = 1)}\Bigg)};
\end{equation}

$\bullet$ likelihood ratio (LR) where 

\begin{equation}
LR_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}) = \Bigg(\frac{W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}|u_{i} = 0)} {W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}^{i-1}}|u_{i} = 1)}\Bigg);
\end{equation}

$\bullet$ log-likelihood (LL) where 

\begin{equation}
LL(y_{1}^{N},\hat{u_{1}}^{i-1}) = \Big[ln\Big(W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}}^{i-1}|u_{i} = 0)\Big), ln\Big(W_{N}^{(i)}(y_{1}^{N},\hat{u_{1}^{i-1}}|u_{i} = 1)\Big)\Big];
\end{equation}


Decoding metric computed from LLR's exhibit better numerical stability than those from LR's or LL's, so we have used the LLR's metric throughout this work. There are different ways to view and understand the operation of SC decoder. In work decoding is viewed as message passing algorithm on an binary tree with $\log(N)$ levels. Decoding is performed by traversing a tree from root to leaf node. Process of decoding involves check node(CN), variable node (VN) operations and threshold detection at leaf node. Decoder receives a LLR value for every bit which needs to decoded (including both frozen and information bits), in other words for code with block length $N$ SC decoder receives $N$ LLR values. Decoding process estimates the bits $\hat{u}_{i} $  where $i = 0,1,2...,(N-1)$. The decoding tree for $N = 8$ looks as shown in the Figure ~\ref{fig:decodingTree}.

\begin{figure}[h]
	\centering
	\includegraphics{./figures/decodingTree.pdf}
	\caption{Decoding tree}
	\label{fig:decodingTree}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics{./figures/messagePassingDiaS.pdf}
	\caption{Local Decoder}
	\label{fig:msgPassingDia}
\end{figure}

In a decoding  tree, the messages to left child node computed with CN and to the right with VN operation.  Figure ~\ref{fig:msgPassingDia} shows how the messages are exchanged in a local component decoder.

The CN and VN operations in LLR domain are given by following equations:

$\bullet$ Check Node (CN) operation

\begin{equation} \label{cnop}
	\alpha_{v_{l}}[i] = \alpha_{v}[i] + \alpha_{v}[i + N_{v}/2]
\end{equation}

$\bullet$ Variable Node (VN) operation

\begin{equation} \label{vnop}
\alpha_{v_{r}}[i] = \alpha_{v}[i + N_{v}/2] + (1 - 2\beta_{v_{l}}[i]) * \alpha_{v}[i]
\end{equation}

After decoding is done at both right and left child nodes the bits are combined at common parent node. the bit combining operation is given by the following equation.

\begin{equation*} \label{bitCombination}
\beta_{v}[i] = \begin{cases}
				\beta_{v_{l}}[i] \oplus \beta_{v_{r}}[i] & \text{if }i < N_{v}/2 \\
				\beta_{v_{r}}[i]
				\end{cases}
\end{equation*}

In Figure ~\ref{fig:msgPassingDia} and in equations ~\ref{cnop}, ~\ref{vnop}  $\alpha_{v}$, $\beta_{v}$ represent intermediate LLR values and estimated bits at local decoder respectively.


\TODO{provide an example of decoding with real LLR values for $N = 8$}

\paragraph{\emph{B. Improved Successive Cancellation Decoding (fast-SSC)}\newline}  \label{fastSSC} 
In basic SC algorithm decoding is performed sequentially, previously decoded values are used for decoding the present bit. Due to sequential nature of the decoder, decoding latency in high, which is undesirable. \cite{SSC} and \cite{fastSSC} try to identify special kind of nodes in a decoder tree which can immediately decoded without traversing till the end of tree. \cite{SSC} tries to identify node with all information bits or all frozen bits. These nodes are called rate-one($R1$) and rate-zero ($R0$) nodes respectively. The $R1$ node can be decoded by taking hard decision and polar transform, since there is no extra information which can be gained from traversing the tree. Decoding of $R0$ node is not necessary since none of them are information bits, so all the bits are set known value which is known at transmitter and receiver which are mostly set to zero. \par Authors in \cite{fastSSC} extend the idea presented in \cite{SSC} by identifying two more kind of special nodes which can be decoded without traversing the tree single parity check ($SPC$) and repetition ($REP$) nodes. Both in \cite{SSC} and \cite{fastSSC} node type is identified based on the frozen pattern at the component decoder. For $SPC$ node, only one frozen bit is present at left most position. For $REP$ node, frozen pattern contains one information bit at right most position, remaining are frozen bits.

One such example, when frozen indices for $N = 8$ are $\{0,1,3,4\}$. The full decoding tree of Figure ~\ref{fig:decodingTree} gets reduced to tree with fewer nodes as shown in Figure ~\ref{fig:decodingTreePruned}. We can easily see that, in the original decoder tree number of nodes were $15$, in the pruned tree nodes is reduced to 7, which results in a significant reduction in number of computations and decoding latency.

\begin{figure}[h]
	\centering
	\includegraphics{./figures/decodingTreePruned.pdf}
	\caption{Pruned Decoder Tree}
	\label{fig:decodingTreePruned}
\end{figure}


\paragraph{\emph{C. List Decoding of Polar Codes (SCL)}\newline}  \label{SCL}


\section{Processor architecture background \newline}

\TODO{Here I need to explain the typical features which are important for understanding the latency contributors and how they can be resolved}

